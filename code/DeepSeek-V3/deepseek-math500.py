import json
import re
import sys
import time
import os
import requests
from tqdm import tqdm
from collections import defaultdict
import importlib.metadata as metadata
from typing import Optional, Dict, Any, Tuple
import matplotlib.pyplot as plt
import seaborn as sns


DEEPSEEK_API_KEY = "sk-da0025c4e3f84de082271474bd734f96"
API_URL = "https://api.deepseek.com/v1/chat/completions"
MODEL_NAME = "deepseek-chat"
DATA_PATH = "datsets/math500.jsonl"
OUTPUT_PATH = "evaluation_results.json"
MAX_RETRIES = 3
REQUEST_TIMEOUT = 45
ERROR_MARGIN = 0.0001


class StatsCollector:
    def __init__(self):
        self.data = defaultdict(lambda: {'correct': 0, 'total': 0})
    
    def update(self, key: str, is_correct: bool) -> None:
        self.data[key]['total'] += 1
        if is_correct:
            self.data[key]['correct'] += 1
    
    def get_accuracy(self, key: str) -> float:
        return self.data[key]['correct'] / self.data[key]['total'] if self.data[key]['total'] else 0.0

def retry_decorator():
    from tenacity import retry, stop_after_attempt, wait_exponential
    return retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=1, min=5, max=30)
    )

@retry_decorator()
def call_deepseek_api(problem: str) -> str:
    headers = {
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
        "Content-Type": "application/json",
        "User-Agent": "MathEvaluator/2.0"
    }
    payload = {
        "model": MODEL_NAME,
        "messages": [{
            "role": "user",
            "content": f"please give numeric answer directly, questionï¼š{problem}"
        }],
        "temperature": 0.1,
        "max_tokens": 50
    }
    
    try:
        response = requests.post(
            API_URL,
            headers=headers,
            json=payload,
            timeout=REQUEST_TIMEOUT
        )
        
        if response.status_code != 200:
            error_info = {
                "status": response.status_code,
                "headers": dict(response.headers),
                "body": response.text[:500]
            }
            raise requests.HTTPError(f": {json.dumps(error_info, indent=2)}")
            
        return response.json()['choices'][0]['message']['content']
        
    except requests.exceptions.SSLError as e:
        raise RuntimeError(f"SSLè¯ä¹¦é”™è¯¯: {str(e)}")
    except requests.exceptions.ProxyError as e:
        raise RuntimeError(f"ä»£ç†é”™è¯¯: {str(e)}")
    except json.JSONDecodeError as e:
        raise RuntimeError(f"å“åº”è§£æžå¤±è´¥: {str(e)}\n original response: {response.text[:500]}")

def format_answer(value: Optional[float]) -> Tuple[str, str]:
    if value is None:
        return ("N/A", "missing")
    try:
        if abs(value) >= 1e6 or (0 < abs(value) <= 1e-4):
            return (f"{value:.4e}", "scientific")
        return (f"{value:.6g}", "normal")
    except Exception as e:
        return (f"Invalid ({str(e)})", "error")

def extract_number(text: str) -> Optional[float]:
    original_text = text
    text = text.replace('$', '').replace(' ', '').replace(',', '')
    
    patterns = [
        (r'([-+]?\d*\.?\d+[eE][-+]?\d+)', 'scientific'),
        (r'\\boxed{([^{}]+)}', 'latex'),
        (r'(\d+\.?\d*%)', 'percentage'),
        (r'â‰ˆ\s*([\d.]+)', 'approximate'),
        (r'([-+]?\d+/\d+)', 'fraction'),
        (r'final result[ï¼š:\s]*([-+Â±]?\d*\.?\d+)', 'chinese'),
        (r'answer[\s:]*([-+Â±]?\d*\.?\d+)', 'english')
    ]
    
    for pattern, ptype in patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        if matches:
            value = matches[-1][0] if isinstance(matches[-1], tuple) else matches[-1]

            try:
                if ptype == 'scientific':
                    return float(value)
                elif ptype == 'percentage':
                    return float(value.strip('%')) / 100
                elif ptype == 'fraction':
                    numerator, denominator = map(float, value.split('/'))
                    return numerator / denominator
                else:
                    return float(value)
            except (ValueError, ZeroDivisionError):
                continue
    
    try:
        numbers = re.findall(r'[-+]?\d*\.?\d+', text)
        if numbers:
            return float(numbers[-1])
    except:
        pass
    
    return None

def is_answer_correct(model_num: Optional[float], ref_num: Optional[float]) -> bool:
    try:
        if None in (model_num, ref_num):
            return False
            
        model = float(model_num)
        ref = float(ref_num)
        
        if model == ref == 0:
            return True
            
        abs_diff = abs(model - ref)
        rel_diff = abs_diff / (abs(ref) + 1e-9)
        return abs_diff <= ERROR_MARGIN or rel_diff <= ERROR_MARGIN
    except (TypeError, ValueError):
        return False

def check_environment():
    required_packages = {
        'tenacity': '8.2.0',
        'requests': '2.32.3',
        'tqdm': '4.67.1',
        'setuptools': '68.2.2',
        'matplotlib': '3.8.2',
        'seaborn': '0.13.2'
    }
    
    print("ðŸ” æ­£åœ¨æ‰§è¡ŒçŽ¯å¢ƒæ£€æŸ¥...")
    missing_packages = []
    version_mismatch = []
    
    for pkg, required_ver in required_packages.items():
        try:
            installed_ver = metadata.version(pkg)
            if installed_ver != required_ver:
                version_mismatch.append(f"{pkg} éœ€è¦ {required_ver}ï¼Œå½“å‰ {installed_ver}")
        except metadata.PackageNotFoundError:
            missing_packages.append(pkg)
    
    if missing_packages or version_mismatch:
        print("âŒ çŽ¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼š")
        if missing_packages:
            print(f"æœªå®‰è£…çš„åŒ…: {', '.join(missing_packages)}")
        if version_mismatch:
            print(f"ç‰ˆæœ¬ä¸åŒ¹é…:\n  - " + "\n  - ".join(version_mismatch))
        print("\nðŸ’¡ è§£å†³æ–¹æ¡ˆ: æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–")
        print("pip install \\")
        print(f"  tenacity=={required_packages['tenacity']} \\")
        print(f"  requests=={required_packages['requests']} \\")
        print(f"  tqdm=={required_packages['tqdm']} \\")
        print(f"  setuptools=={required_packages['setuptools']} \\")
        print(f"  matplotlib=={required_packages['matplotlib']} \\")
        print(f"  seaborn=={required_packages['seaborn']}")
        sys.exit(1)
    
    print("âœ… çŽ¯å¢ƒæ£€æŸ¥é€šè¿‡")

# ======================
# å¯è§†åŒ–æ¨¡å—
# ======================
def generate_visualizations(report: dict, output_dir: str = "visualization_results"):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    sns.set_theme(style="whitegrid", palette="husl")

    # å­¦ç§‘å‡†ç¡®çŽ‡æŸ±çŠ¶å›¾
    plt.figure(figsize=(12, 6))
    subjects = list(report['stats']['by_subject'].keys())
    accuracies = [v*100 for v in report['stats']['by_subject'].values()]
    sns.barplot(x=subjects, y=accuracies)
    plt.title('å­¦ç§‘å‡†ç¡®çŽ‡å¯¹æ¯”')
    plt.ylabel('å‡†ç¡®çŽ‡ (%)')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f'{output_dir}/subject_accuracy.png', dpi=300)
    plt.close()

    # éš¾åº¦åˆ†å¸ƒé¥¼å›¾
    plt.figure(figsize=(8, 8))
    levels = list(report['stats']['by_level'].keys())
    counts = [report['stats']['by_level'][lvl]['total'] for lvl in levels]
    plt.pie(counts, labels=levels, autopct='%1.1f%%', startangle=90)
    plt.title('é¢˜ç›®éš¾åº¦åˆ†å¸ƒ')
    plt.savefig(f'{output_dir}/level_distribution.png', dpi=300)
    plt.close()

    # ç­”æ¡ˆç±»åž‹åˆ†å¸ƒå¯¹æ¯”
    fig, ax = plt.subplots(1, 2, figsize=(15, 6))
    model_types = report['stats']['answer_types']['model']
    ref_types = report['stats']['answer_types']['reference']
    
    sns.barplot(x=list(model_types.keys()), y=list(model_types.values()), ax=ax[0])
    ax[0].set_title('æ¨¡åž‹ç­”æ¡ˆç±»åž‹åˆ†å¸ƒ')
    ax[0].set_ylabel('æ•°é‡')
    
    sns.barplot(x=list(ref_types.keys()), y=list(ref_types.values()), ax=ax[1])
    ax[1].set_title('å‚è€ƒç­”æ¡ˆç±»åž‹åˆ†å¸ƒ')
    plt.tight_layout()
    plt.savefig(f'{output_dir}/answer_type_comparison.png', dpi=300)
    plt.close()

    print(f"\nðŸ“ˆ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜è‡³ {output_dir} ç›®å½•")

# ======================
# ä¸»è¯„ä¼°æµç¨‹
# ======================
def run_evaluation():
    check_environment()
    
    stats = {
        'global': {'correct': 0, 'total': 0},
        'subjects': StatsCollector(),
        'levels': StatsCollector(),
        'answer_types': {'model': defaultdict(int), 'reference': defaultdict(int)}
    }
    results = []
    
    with open(DATA_PATH, 'r', encoding='utf-8') as f:
        dataset = [json.loads(line) for line in f if line.strip()]
    
    # æ–°å¢žæ•°æ®æ¸…æ´—
    dataset = [item for item in dataset if item.get('problem') and item.get('answer')]
    print(f"æ¸…æ´—åŽæœ‰æ•ˆæ•°æ®é‡ï¼š{len(dataset)}")

    progress_bar = tqdm(dataset, desc="ðŸ”§ è¯„ä¼°è¿›åº¦", 
                      bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [å‡†ç¡®çŽ‡: {postfix}]")

    for item in progress_bar:
        try:
            raw_response = call_deepseek_api(item['problem'])
            model_answer = extract_number(raw_response)
            ref_answer = extract_number(item['answer'])
            correct = is_answer_correct(model_answer, ref_answer)
            
            # æ ¼å¼åŒ–æ˜¾ç¤º
            model_display, model_type = format_answer(model_answer)
            ref_display, ref_type = format_answer(ref_answer)
            
            # è®°å½•ç»“æžœï¼ˆç»Ÿä¸€å­—æ®µåï¼‰
            record = {
                'problem': item['problem'],
                'model_raw': model_answer,
                'reference_raw': ref_answer,
                'model_display': model_display,
                'reference_display': ref_display,
                'model_type': model_type,
                'reference_type': ref_type,
                'is_correct': correct,  # ä¿®æ”¹å­—æ®µå
                'subject': item.get('subject', 'unknown'),
                'level': item.get('level', 'unknown'),
                'raw_response': raw_response[:200] + '...'
            }
            results.append(record)
            
            # æ›´æ–°ç»Ÿè®¡
            stats['global']['total'] += 1
            stats['global']['correct'] += int(correct)
            stats['subjects'].update(record['subject'], correct)
            stats['levels'].update(record['level'], correct)
            stats['answer_types']['model'][record['model_type']] += 1
            stats['answer_types']['reference'][record['reference_type']] += 1
            
            # å®žæ—¶æ˜¾ç¤º
            current_acc = stats['global']['correct'] / stats['global']['total']
            progress_bar.set_postfix_str(f"{current_acc:.1%}")
            tqdm.write(
                f"{'âœ…' if correct else 'âŒ'} {item['problem'][:40]}... | "
                f"å‚è€ƒ: {ref_display} vs æ¨¡åž‹: {model_display}"
            )
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            tqdm.write(f"ðŸ”¥ {error_msg}")
            results.append({
                'problem': item['problem'],
                'error': error_msg,
                'is_correct': False  # ç»Ÿä¸€å­—æ®µå
            })

    # ç”ŸæˆæŠ¥å‘Š
    report = {
        'metadata': {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'data_size': len(dataset),
            'config': {
                'model': MODEL_NAME,
                'timeout': REQUEST_TIMEOUT,
                'error_margin': ERROR_MARGIN
            }
        },
        'stats': {
            'global_accuracy': stats['global']['correct'] / stats['global']['total'],
            'by_subject': {s: stats['subjects'].get_accuracy(s) for s in stats['subjects'].data},
            'by_level': {l: stats['levels'].get_accuracy(l) for l in stats['levels'].data},
            'answer_types': dict(stats['answer_types'])
        },
        'details': results
    }
    
    with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    generate_visualizations(report)
    
    print("\nðŸ“Š è¯„ä¼°æŠ¥å‘Š")
    print(f"æ€»é¢˜æ•°: {report['metadata']['data_size']}")
    print(f"æ€»å‡†ç¡®çŽ‡: {report['stats']['global_accuracy']:.2%}")
    
    return report

# ======================
# æµ‹è¯•æ¨¡å—
# ======================
def test_extractor():
    """è§£æžå™¨æµ‹è¯•"""
    test_cases = [
        ("ç­”æ¡ˆæ˜¯\\boxed{3.14}", 3.14),
        ("ç»“æžœçº¦5%", 0.05),
        ("1.23e5", 123000.0),
        ("ç­”æ¡ˆï¼šâ‰ˆ3.1416", 3.1416),
        ("11/2", 5.5),
        ("\\mathbf{6.022e23}", 6.022e23),
        ("æ— æ•ˆç­”æ¡ˆ", None),
        ("æœ€åŽæ•°å€¼æ˜¯42", 42.0)
    ]
    
    print("\nðŸ”¬ æ­£åœ¨æ‰§è¡Œè§£æžæµ‹è¯•...")
    print(f"æµ‹è¯•çŽ¯å¢ƒç‰ˆæœ¬: tenacity {metadata.version('tenacity')}")
    
    for text, expected in test_cases:
        result = extract_number(text)
        success = abs(result - expected) < 1e-6 if (result and expected) else result == expected
        display, _ = format_answer(result)
        exp_display, _ = format_answer(expected)
        status = "âœ…" if success else "âŒ"
        print(f"{status} {text} â†’ {display} (é¢„æœŸ: {exp_display})")
    
    print("ðŸŽ‰ æ‰€æœ‰è§£æžæµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_extractor()
    run_evaluation()
