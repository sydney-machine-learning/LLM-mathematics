{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13d41d2-6496-4532-853d-367dcde55b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ Initialize OpenAI client (recommended to use OPENAI_API_KEY as an environment variable)\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.Client(api_key=API_KEY)\n",
    "\n",
    "# ‚úÖ Load the CSV containing sub-questions and background\n",
    "df = pd.read_csv(\"unsw.csv\")\n",
    "\n",
    "# ‚úÖ Prompt template with full background\n",
    "\n",
    "def build_prompt(full_question, subquestion):\n",
    "    return f\"\"\"You are a statistics master student.\n",
    "\n",
    "Here is the full background of the problem:\n",
    "{full_question}\n",
    "\n",
    "Now, please solve the following sub-question step-by-step using structured reasoning:\n",
    "\n",
    "{subquestion}\n",
    "\n",
    "For each step, return a JSON object with:\n",
    "- step: the step number (or \"final\" if it's the final answer),\n",
    "- desc: a short description of what you are doing,\n",
    "- expr: a math expression if applicable,\n",
    "- value: the computed result if any\n",
    "\n",
    "Format the full output as a JSON array of steps.\n",
    "Return ONLY a valid JSON array. No explanations. No markdown.\n",
    "\"\"\"\n",
    "\n",
    "# ‚úÖ GPT call function (with debug-friendly saving + error tolerance)\n",
    "def call_gpt4o(prompt, qid, subid, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Clean markdown wrappers\n",
    "        if reply.startswith(\"```json\"):\n",
    "            reply = reply.removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
    "        elif reply.startswith(\"```\"):\n",
    "            reply = reply.removeprefix(\"```\").removesuffix(\"```\").strip()\n",
    "\n",
    "        return json.loads(reply)\n",
    "\n",
    "    except json.JSONDecodeError as jde:\n",
    "        print(f\"‚ùå JSON decoding failed ({qid}-{subid}):\", jde)\n",
    "        return [{\"step\": \"error\", \"desc\": \"Invalid JSON output\", \"raw\": reply[:300]}]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API call failed ({qid}-{subid}):\", e)\n",
    "        return [{\"step\": \"error\", \"desc\": str(e)}]\n",
    "\n",
    "# ‚úÖ Main loop: iterate through each sub-question\n",
    "solutions = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    qid = row[\"qid\"]\n",
    "    subid = row[\"subid\"]\n",
    "    subq = row[\"subquestion\"]\n",
    "    fullq = row[\"full_question\"]\n",
    "\n",
    "    print(f\"\\nüß† Solving: {qid}-{subid}\")\n",
    "\n",
    "    prompt = build_prompt(fullq, subq)\n",
    "    steps = call_gpt4o(prompt, qid, subid)\n",
    "\n",
    "    solutions.append({\n",
    "        \"qid\": qid,\n",
    "        \"subid\": subid,\n",
    "        \"subquestion\": subq,\n",
    "        \"full_question\": fullq,\n",
    "        \"steps\": steps\n",
    "    })\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "# ‚úÖ Save all results\n",
    "with open(\"gpt4o_subquestion_solutions.json\", \"w\") as f:\n",
    "    json.dump(solutions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n‚úÖ All sub-questions processed! Results saved to gpt4o_subquestion_solutions.json\")\n",
    "\n",
    "import math\n",
    "\n",
    "# ‚úÖ Recursively convert NaN to None (valid JSON null)\n",
    "def clean_nan(obj):\n",
    "    if isinstance(obj, float) and math.isnan(obj):\n",
    "        return None\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_nan(x) for x in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: clean_nan(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# ‚úÖ Apply to the full solutions list\n",
    "cleaned_solutions = clean_nan(solutions)\n",
    "\n",
    "# ‚úÖ Save as valid JSON\n",
    "with open(\"gpt4o_subquestion_solutions.json\", \"w\") as f:\n",
    "    json.dump(cleaned_solutions, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n‚úÖ Cleaned and saved (NaN ‚Üí null). File written: gpt4o_subquestion_solutions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c77bf-eed0-478c-8e25-40a2c954b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "DEEPSEEK_API_URL = \"https://api.deepseek.com/v1/chat/completions\"  # Verify endpoint\n",
    "\n",
    "# Load GPT-generated solutions\n",
    "with open(\"gpt4o_subquestion_solutions.json\") as f:\n",
    "    solutions = json.load(f)\n",
    "\n",
    "# Grading prompt template\n",
    "def build_grading_prompt(subquestion, steps_json):\n",
    "    steps_str = json.dumps(steps_json, indent=2, ensure_ascii=False)\n",
    "    return f\"\"\"You are a statistics tutor. Please grade a student's step-by-step solution to a sub-question.\n",
    "\n",
    "Sub-question:\n",
    "{subquestion}\n",
    "\n",
    "Student's steps:\n",
    "{steps_str}\n",
    "\n",
    "Now, do the following:\n",
    "1. Evaluate if each step is correct or flawed. If flawed, explain why.\n",
    "2. Give a short comment for each step.\n",
    "3. Give an overall score out of 5 and a short feedback.\n",
    "4. Use the following scoring rubric:\n",
    "   1 - Completely incorrect: Major logical flaws, fundamental misunderstandings, or missing core steps. Lacks basic understanding.\n",
    "   2 - Weak: Some grasp of the method, but contains multiple errors, flawed reasoning, or incoherent structure.\n",
    "   3 - Satisfactory: Main method is correct, includes key steps, but has some calculation or explanation issues.\n",
    "   4 - Good: Mostly correct, logically structured, only minor issues such as small errors or slightly informal reasoning.\n",
    "   5 - Excellent: Fully correct, well-organized, rigorous and clear reasoning. A model solution.\n",
    "5. Format your output as the following JSON:\n",
    "\n",
    "{{\n",
    "  \"score\": X,\n",
    "  \"total\": 5,\n",
    "  \"feedback\": \"...\",\n",
    "  \"step_feedback\": [{{\"step\": ..., \"comment\": \"...\"}}, ...]\n",
    "}}\n",
    "Return ONLY a valid JSON object. No markdown, no extra explanation.\n",
    "\"\"\"\n",
    "\n",
    "# Modified DeepSeek grading function\n",
    "def grade_with_deepseek(question, steps, qid, subid, model=\"deepseek-chat\"):\n",
    "    prompt = build_grading_prompt(question, steps)\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {DEEPSEEK_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "\n",
    "        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        reply = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        \n",
    "        # Clean markdown formatting\n",
    "        if reply.startswith(\"```json\"):\n",
    "            reply = reply[7:-3].strip()\n",
    "        elif reply.startswith(\"```\"):\n",
    "            reply = reply[3:-3].strip()\n",
    "\n",
    "        return json.loads(reply)\n",
    "\n",
    "    except json.JSONDecodeError as jde:\n",
    "        print(f\"‚ùå JSON parsing failed ({qid}-{subid}):\", jde)\n",
    "        return {\"score\": 0, \"total\": 5, \"feedback\": \"Invalid JSON output\", \"step_feedback\": []}\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        print(f\"‚ùå API request failed ({qid}-{subid}):\", re)\n",
    "        return {\"score\": 0, \"total\": 5, \"feedback\": f\"API Error: {str(re)}\", \"step_feedback\": []}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error ({qid}-{subid}):\", e)\n",
    "        return {\"score\": 0, \"total\": 5, \"feedback\": str(e), \"step_feedback\": []}\n",
    "\n",
    "# Batch grading with DeepSeek\n",
    "graded_results = []\n",
    "for item in solutions:\n",
    "    qid = item[\"qid\"]\n",
    "    subid = item[\"subid\"]\n",
    "    question = item[\"subquestion\"]\n",
    "    steps = item[\"steps\"]\n",
    "\n",
    "    print(f\"üìù Grading: {qid}-{subid}\")\n",
    "    grading = grade_with_deepseek(question, steps, qid, subid)\n",
    "\n",
    "    graded_results.append({\n",
    "        \"qid\": qid,\n",
    "        \"subid\": subid,\n",
    "        \"subquestion\": question,\n",
    "        \"score\": grading.get(\"score\", 0),\n",
    "        \"total\": grading.get(\"total\", 5),\n",
    "        \"feedback\": grading.get(\"feedback\", \"\"),\n",
    "        \"step_feedback\": grading.get(\"step_feedback\", [])\n",
    "    })\n",
    "\n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# Save grading results\n",
    "with open(\"deepseek_grading_results.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(graded_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ All sub-questions graded and saved as deepseek_grading_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67897e60-6ab0-498e-b762-8166e2d5509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read grading results\n",
    "with open(\"deepseek_grading_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    graded = json.load(f)\n",
    "\n",
    "# Filter out sub-questions that did not receive full marks, and record their original indices\n",
    "wrong = []\n",
    "for idx, item in enumerate(graded):\n",
    "    if item.get(\"score\", 0) < item.get(\"total\", 5):\n",
    "        item_with_index = item.copy()\n",
    "        item_with_index[\"index\"] = idx  # Add original index\n",
    "        wrong.append(item_with_index)\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"wrong_subquestions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(wrong, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(wrong)} incorrect sub-questions and saved to wrong_subquestions.json with original indices recorded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50250f92-c6a1-450d-84fe-703ef851bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Load grading results\n",
    "with open(\"deepseek_grading_results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    graded = json.load(f)\n",
    "\n",
    "# ‚úÖ Define index ranges for each course category\n",
    "category_ranges = {\n",
    "    \"Statistical Inference\": range(0, 39),\n",
    "    \"Computational Methods for Finance\": range(39, 119),\n",
    "    \"Optimization\": range(119, 134)\n",
    "}\n",
    "\n",
    "# ‚úÖ Initialize statistics container\n",
    "category_distribution = defaultdict(lambda: {\"score_counts\": Counter(), \"total\": 0})\n",
    "\n",
    "# ‚úÖ Iterate over grading results and categorize by index\n",
    "for idx, item in enumerate(graded):\n",
    "    for category, index_range in category_ranges.items():\n",
    "        if idx in index_range:\n",
    "            score = item.get(\"score\", 0)\n",
    "            total_score = item.get(\"total\", 5)\n",
    "            category_distribution[category][\"score_counts\"][score] += 1\n",
    "            category_distribution[category][\"total\"] += 1\n",
    "            break\n",
    "\n",
    "# ‚úÖ Build summary table\n",
    "summary_data = []\n",
    "for category, data in category_distribution.items():\n",
    "    total_questions = data[\"total\"]\n",
    "    full_score = max(data[\"score_counts\"].keys(), default=5)\n",
    "    accuracy = data[\"score_counts\"].get(full_score, 0) / total_questions if total_questions > 0 else 0\n",
    "\n",
    "    row = {\n",
    "        \"Category\": category,\n",
    "        \"Total Questions\": total_questions,\n",
    "        \"Accuracy\": f\"{accuracy:.2%}\"\n",
    "    }\n",
    "\n",
    "    # Add count and proportion for each score level\n",
    "    for score in range(full_score + 1):\n",
    "        count = data[\"score_counts\"].get(score, 0)\n",
    "        row[f\"Score {score} Count\"] = count\n",
    "        row[f\"Score {score} Ratio\"] = f\"{(count / total_questions):.1%}\" if total_questions > 0 else \"0%\"\n",
    "\n",
    "    summary_data.append(row)\n",
    "\n",
    "# ‚úÖ Convert to DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# ‚úÖ Custom subject order and score levels\n",
    "custom_order = [\"Optimization\", \"Computational Methods for Finance\", \"Statistical Inference\"]\n",
    "score_levels = [1, 2, 3, 4, 5]\n",
    "\n",
    "# ‚úÖ Extract ratio data for plotting\n",
    "subject_ratios = {\n",
    "    cat: [\n",
    "        float(df_summary[df_summary[\"Category\"] == cat][f\"Score {score} Ratio\"].values[0].strip('%')) / 100\n",
    "        for score in score_levels\n",
    "    ]\n",
    "    for cat in custom_order\n",
    "}\n",
    "\n",
    "# ‚úÖ Convert to matrix (score as x-axis)\n",
    "ratios_matrix = np.array(list(subject_ratios.values())).T  # shape: (num_scores, num_categories)\n",
    "\n",
    "# ‚úÖ Plot settings\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(score_levels))\n",
    "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green ‚Äî match custom_order\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# ‚úÖ Draw bar chart for each subject\n",
    "for i, cat in enumerate(custom_order):\n",
    "    offsets = x + (i - 1) * bar_width\n",
    "    heights = ratios_matrix[:, i]\n",
    "    bars = plt.bar(offsets, heights, width=bar_width, label=cat, color=colors[i])\n",
    "\n",
    "    # ‚úÖ Add percentage labels\n",
    "    for bar, height in zip(bars, heights):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "                 f\"{height:.1%}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# ‚úÖ Chart decoration\n",
    "plt.xticks(x, [str(s) for s in score_levels])\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.title(\"Score Distribution of gpt4o by Subject\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008d4ff-8b5c-4ef2-84a2-518795237a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error types and their counts\n",
    "categories = [\n",
    "    \"Lack of Justification or Explanation\",\n",
    "    \"Less Precise\",\n",
    "    \"Missing Intermediate Steps or Details\",\n",
    "    \"Unclear or Inconsistent Notation\",\n",
    "    \"Poor Structure or Lack of Formal Presentation\",\n",
    "    \"Incomplete Gradient/Hessian Analysis\",\n",
    "    \"Lack of Logical Flow\",\n",
    "    \"Missing Eigenvalue/Matrix-Related Reasoning\"\n",
    "]\n",
    "counts = [26, 22, 6, 5, 2, 2, 2, 1]\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(counts, labels=categories, autopct='%1.1f%%', startangle=140, textprops={'fontsize': 10})\n",
    "plt.title(\"GPT-4o Error Type Distribution on UNSW Dataset\", fontsize=14)\n",
    "plt.axis('equal') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc31552-c502-4981-80be-2b7c9c8fd26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
